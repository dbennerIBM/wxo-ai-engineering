{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete RAG Pipeline: PDF Processing with Docling + Granite Embeddings + Elasticsearch\n",
    "\n",
    "This notebook demonstrates an end-to-end RAG (Retrieval-Augmented Generation) pipeline:\n",
    "1. **PDF Preprocessing** with IBM Docling\n",
    "2. **Embedding Generation** with Granite Embedding models\n",
    "3. **Vector Storage & Search** with Elasticsearch\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- Elasticsearch 8.0+ running locally or remotely\n",
    "- Sufficient RAM for model loading (~2GB minimum)\n",
    "\n",
    "## Architecture Overview\n",
    "```\n",
    "PDF Documents\n",
    "    ‚Üì\n",
    "Docling (Convert to Markdown)\n",
    "    ‚Üì\n",
    "Smart Chunking (Semantic boundaries)\n",
    "    ‚Üì\n",
    "Granite Embedding (Generate vectors)\n",
    "    ‚Üì\n",
    "Elasticsearch (Store & Search)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies\n",
    "\n",
    "**What this does:** Installs all required Python packages for the pipeline.\n",
    "\n",
    "**Packages installed:**\n",
    "- `docling`: IBM's document processing library\n",
    "- `elasticsearch`: Python client for Elasticsearch\n",
    "- `sentence-transformers`: For loading Granite embedding models\n",
    "- `langchain` & `langchain-text-splitters`: For intelligent text chunking\n",
    "- `torch`: PyTorch backend for embeddings\n",
    "\n",
    "**Note:** This may take 2-5 minutes on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -q docling elasticsearch>=8.0<9.0 sentence-transformers langchain langchain-text-splitters torch numpy ocrmac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Libraries\n",
    "\n",
    "**What this does:** Imports all necessary Python libraries and modules.\n",
    "\n",
    "**Key imports:**\n",
    "- Document processing: `DocumentConverter`, `PdfPipelineOptions`\n",
    "- Embeddings: `SentenceTransformer`\n",
    "- Search: `Elasticsearch`\n",
    "- Chunking: `RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbenner/IBM/wxo-ai-engineering/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# Docling imports for PDF processing\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions, \n",
    "    TableFormerMode, \n",
    "    TableStructureOptions,\n",
    "    TesseractOcrOptions,\n",
    "    OcrMacOptions\n",
    ")\n",
    "\n",
    "# LangChain for text chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Sentence Transformers for Granite embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Elasticsearch for vector storage\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration Variables\n",
    "\n",
    "**What this does:** Sets up all configurable parameters for the pipeline.\n",
    "\n",
    "**Configuration Options:**\n",
    "\n",
    "### Elasticsearch Settings\n",
    "- `ES_HOST`: Elasticsearch server address (default: \"localhost\")\n",
    "- `ES_PORT`: Elasticsearch port (default: 9200)\n",
    "- `ES_USER`: Username if authentication enabled (default: \"elastic\")\n",
    "- `ES_PASSWORD`: Password if authentication enabled (set to None for no auth)\n",
    "- `ES_INDEX_NAME`: Name of the index to create (default: \"docling_granite_rag\")\n",
    "\n",
    "### Granite Embedding Model Options\n",
    "Choose one based on your needs:\n",
    "- `granite-embedding-30m-english`: **Fastest**, English only, 30M parameters\n",
    "- `granite-embedding-125m-english`: **Balanced**, English only, 125M parameters (RECOMMENDED)\n",
    "- `granite-embedding-278m-multilingual`: **Best quality**, multilingual, 278M parameters\n",
    "- `granite-embedding-107m-multilingual`: Multilingual, 107M parameters\n",
    "\n",
    "### Document Processing Settings\n",
    "- `ENABLE_OCR`: Enable OCR for scanned PDFs (True/False)\n",
    "- `CHUNK_SIZE`: Maximum characters per chunk (default: 1000)\n",
    "  - Smaller (500-800): More precise retrieval, more chunks\n",
    "  - Larger (1200-1500): More context, fewer chunks\n",
    "- `CHUNK_OVERLAP`: Character overlap between chunks (default: 200)\n",
    "  - Prevents context loss at chunk boundaries\n",
    "- `BATCH_SIZE`: Documents to process at once (default: 50)\n",
    "  - Reduce if memory issues occur\n",
    "\n",
    "### Output Settings\n",
    "- `OUTPUT_DIR`: Directory for intermediate files (default: \"./output\")\n",
    "- `SAVE_INTERMEDIATE`: Save Markdown files (True/False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the ES connection details in the cell below. Here is an example of the format to follow. \n",
    "\n",
    "üö® Note: The ES_HOST is just the url minus the protocol prefix. \n",
    "\n",
    "![es connection details example](resources/es_connection_credentials.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Elasticsearch: :\n",
      "  Index Name: docling_granite_rag\n",
      "  Model: ibm-granite/granite-embedding-125m-english\n",
      "  Chunk Size: 1000 chars\n",
      "  Chunk Overlap: 200 chars\n",
      "  OCR Enabled: True\n",
      "  Output Directory: ./output\n",
      "‚úì Configuration complete\n"
     ]
    }
   ],
   "source": [
    "# ===== ELASTICSEARCH CONFIGURATION =====\n",
    "from psutil import MACOS\n",
    "\n",
    "\n",
    "ES_HOST = \"\"  # Change to your Elasticsearch host\n",
    "ES_PORT = \"31972\"         # Change to your Elasticsearch port\n",
    "ES_USER = \"\"    # Change if using different username\n",
    "ES_PASSWORD = \"\"    # Set password here if authentication is enabled, e.g., \"your_password\"\n",
    "ES_INDEX_NAME = \"docling_granite_rag\"  # Name of the Elasticsearch index\n",
    "\n",
    "# ===== GRANITE EMBEDDING MODEL CONFIGURATION =====\n",
    "# Choose your model based on requirements:\n",
    "# - For speed: granite-embedding-30m-english\n",
    "# - For balance: granite-embedding-125m-english (RECOMMENDED)\n",
    "# - For multilingual: granite-embedding-278m-multilingual\n",
    "GRANITE_MODEL = \"ibm-granite/granite-embedding-125m-english\"\n",
    "\n",
    "# ===== DOCUMENT PROCESSING CONFIGURATION =====\n",
    "ENABLE_OCR = True      # Enable OCR for scanned PDFs\n",
    "MACOS = True            # Enables OcrMacOptions() instead of TesseractOcrOptions(); Set to true if running on macos to get faster more accurate OCR with Apple's native Vision framework. \n",
    "CHUNK_SIZE = 1000      # Characters per chunk (adjust based on your needs)\n",
    "CHUNK_OVERLAP = 200    # Overlap between chunks to maintain context\n",
    "BATCH_SIZE = 50        # Number of documents to process in one batch\n",
    "\n",
    "# ===== OUTPUT CONFIGURATION =====\n",
    "OUTPUT_DIR = \"./output\"              # Directory for intermediate files\n",
    "SAVE_INTERMEDIATE = True             # Save Markdown files for inspection\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Elasticsearch: {ES_HOST}:{ES_PORT}\")\n",
    "print(f\"  Index Name: {ES_INDEX_NAME}\")\n",
    "print(f\"  Model: {GRANITE_MODEL}\")\n",
    "print(f\"  Chunk Size: {CHUNK_SIZE} chars\")\n",
    "print(f\"  Chunk Overlap: {CHUNK_OVERLAP} chars\")\n",
    "print(f\"  OCR Enabled: {ENABLE_OCR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(\"‚úì Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize Docling PDF Processor\n",
    "\n",
    "**What this does:** Creates a Docling document converter with configured options.\n",
    "\n",
    "**Key features:**\n",
    "- Converts PDFs to Markdown while preserving structure\n",
    "- Handles tables, images, and complex layouts\n",
    "- Optional OCR for scanned documents\n",
    "- Supports both local files and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PDF pipeline options\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "\n",
    "# Enable OCR if configured\n",
    "if ENABLE_OCR:\n",
    "    if MACOS:\n",
    "        pipeline_options.ocr_options = OcrMacOptions()\n",
    "        print(\"OCR enabled for scanned documents (using macOS Vision)\")\n",
    "    else:\n",
    "        pipeline_options.ocr_options = TesseractOcrOptions()\n",
    "        print(\"OCR enabled for scanned documents (using Tesseract)\")\n",
    "\n",
    "# Enable table structure processing with Granite model\n",
    "pipeline_options.table_structure_options = TableStructureOptions(\n",
    "        mode=TableFormerMode.ACCURATE,  # Use accurate mode for better quality\n",
    "        do_cell_matching=True\n",
    "    )\n",
    "print(\"IBM Granite 4 Docling enabled for scanned documents\")\n",
    "\n",
    "# Initialize Docling converter\n",
    "docling_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "            # The model will be automatically downloaded from HuggingFace\n",
    "            table_structure_model=\"ibm-granite/granite-docling-258M\"\n",
    "            )\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Docling PDF processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Initialize Granite Embedding Model\n",
    "\n",
    "**What this does:** Loads the Granite embedding model into memory.\n",
    "\n",
    "**Performance notes:**\n",
    "- First run: Downloads model (~50-500MB depending on model size)\n",
    "- Subsequent runs: Loads from cache (fast)\n",
    "- Memory usage:\n",
    "  - 30M model: ~500MB RAM\n",
    "  - 125M model: ~1GB RAM\n",
    "  - 278M model: ~2GB RAM\n",
    "\n",
    "**Model capabilities:**\n",
    "- Generates dense vector embeddings\n",
    "- Optimized for semantic search and RAG\n",
    "- Normalized embeddings (cosine similarity ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading Granite Embedding model: {GRANITE_MODEL}\")\n",
    "print(\"This may take a moment on first run...\")\n",
    "\n",
    "# Load the Granite embedding model\n",
    "embedding_model = SentenceTransformer(GRANITE_MODEL)\n",
    "\n",
    "# Get embedding dimension for Elasticsearch configuration\n",
    "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"‚úì Model loaded successfully\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  Model ready for encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Initialize Elasticsearch Connection\n",
    "\n",
    "**What this does:** Establishes connection to Elasticsearch and verifies it's working.\n",
    "\n",
    "**Connection modes:**\n",
    "- **With authentication**: Uses ES_USER and ES_PASSWORD\n",
    "- **Without authentication**: Direct connection (local dev setups)\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If connection fails, verify Elasticsearch is running: `curl http://localhost:9200`\n",
    "- Check firewall settings if using remote Elasticsearch\n",
    "- Verify credentials if authentication is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Connecting to Elasticsearch at https://{ES_HOST}:{ES_PORT}...\")\n",
    "\n",
    "# Initialize Elasticsearch client with or without authentication\n",
    "if ES_PASSWORD:\n",
    "    es_client = Elasticsearch(\n",
    "        f\"https://{ES_HOST}:{ES_PORT}\",\n",
    "        basic_auth=(ES_USER, ES_PASSWORD),\n",
    "        verify_certs=True, # Set to True if using valid SSL certificates\n",
    "        ca_certs=\"es_certs/cert.pem\",\n",
    "        request_timeout=300\n",
    "    )\n",
    "    print(f\"Connecting with authentication (user: {ES_USER})\")\n",
    "else:\n",
    "    es_client = Elasticsearch([f\"http://{ES_HOST}:{ES_PORT}\"])\n",
    "    print(\"Connecting without authentication\")\n",
    "\n",
    "# Test connection\n",
    "if es_client.ping():\n",
    "    info = es_client.info()\n",
    "    print(f\"‚úì Connected to Elasticsearch {info['version']['number']}\")\n",
    "    print(f\"  Cluster: {info['cluster_name']}\")\n",
    "else:\n",
    "    print(\"‚úó Failed to connect to Elasticsearch\")\n",
    "    print(\"  Please verify Elasticsearch is running and configuration is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Create Elasticsearch Index\n",
    "\n",
    "**What this does:** Creates an Elasticsearch index with proper mappings for vector search.\n",
    "\n",
    "**Index structure:**\n",
    "- `text`: Full-text field for keyword search (type: text)\n",
    "- `embedding`: Dense vector field for semantic search (type: dense_vector)\n",
    "- `metadata`: Flexible object for document metadata (type: object)\n",
    "- `doc_id`: Unique identifier (type: keyword)\n",
    "- `chunk_id`: Chunk position in document (type: integer)\n",
    "- `source`: Original document name (type: keyword)\n",
    "\n",
    "**Vector settings:**\n",
    "- Dimension: Matches Granite model output\n",
    "- Similarity: Cosine (best for normalized embeddings)\n",
    "- Indexing: Enabled for fast kNN search\n",
    "\n",
    "**Warning:** Setting `delete_if_exists=True` will DELETE any existing index with the same name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Set to True to delete existing index\n",
    "DELETE_IF_EXISTS = True  # CHANGE TO True IF YOU WANT TO RECREATE THE INDEX\n",
    "\n",
    "# Delete existing index if configured\n",
    "if DELETE_IF_EXISTS and es_client.indices.exists(index=ES_INDEX_NAME):\n",
    "    es_client.indices.delete(index=ES_INDEX_NAME)\n",
    "    print(f\"‚ö† Deleted existing index: {ES_INDEX_NAME}\")\n",
    "\n",
    "# Define index mapping\n",
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            # Full text for keyword search\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            # Vector embedding for semantic search\n",
    "            \"embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": embedding_dim,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            # Document metadata\n",
    "            \"metadata\": {\n",
    "                \"type\": \"object\",\n",
    "                \"enabled\": True\n",
    "            },\n",
    "            # Unique document identifier\n",
    "            \"doc_id\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            # Chunk position in document\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            # Source document name\n",
    "            \"source\": {\n",
    "                \"type\": \"keyword\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if not es_client.indices.exists(index=ES_INDEX_NAME):\n",
    "    es_client.indices.create(index=ES_INDEX_NAME, body=index_mapping)\n",
    "    print(f\"‚úì Created index: {ES_INDEX_NAME}\")\n",
    "    print(f\"  Vector dimensions: {embedding_dim}\")\n",
    "    print(f\"  Similarity metric: cosine\")\n",
    "else:\n",
    "    print(f\"‚úì Index already exists: {ES_INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Define Helper Functions\n",
    "\n",
    "**What this does:** Creates reusable functions for the pipeline.\n",
    "\n",
    "**Functions defined:**\n",
    "\n",
    "### 1. `convert_pdf_to_markdown(pdf_path, save_output=True)`\n",
    "- Converts PDF to Markdown format\n",
    "- Preserves document structure\n",
    "- Optionally saves to file\n",
    "\n",
    "### 2. `chunk_markdown(markdown_text, chunk_size, chunk_overlap)`\n",
    "- Splits Markdown into semantic chunks\n",
    "- Respects document structure (headers, paragraphs)\n",
    "- Maintains context with overlap\n",
    "\n",
    "### 3. `generate_embeddings(texts)`\n",
    "- Converts text to vector embeddings\n",
    "- Batch processing for efficiency\n",
    "- Normalized vectors for cosine similarity\n",
    "\n",
    "### 4. `index_chunks_to_elasticsearch(chunks, source_name)`\n",
    "- Stores chunks with embeddings in Elasticsearch\n",
    "- Bulk indexing for performance\n",
    "- Adds metadata for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_markdown(pdf_path: str, save_output: bool = SAVE_INTERMEDIATE) -> str:\n",
    "    \"\"\"\n",
    "    Convert PDF to Markdown using Docling with Granite model.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file or URL\n",
    "        save_output: Whether to save Markdown to file\n",
    "    \n",
    "    Returns:\n",
    "        Markdown string\n",
    "    \"\"\"\n",
    "    print(f\"Converting PDF with Granite Docling model: {pdf_path}\")\n",
    "    \n",
    "    # Convert document\n",
    "    result = docling_converter.convert(pdf_path)\n",
    "    markdown_text = result.document.export_to_markdown()\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_output:\n",
    "        pdf_name = Path(pdf_path).stem if not pdf_path.startswith('http') else 'downloaded_pdf'\n",
    "        output_path = Path(OUTPUT_DIR) / f\"{pdf_name}.md\"\n",
    "        output_path.write_text(markdown_text, encoding='utf-8')\n",
    "        print(f\"  Saved Markdown to: {output_path}\")\n",
    "    \n",
    "    print(f\"  Converted {len(markdown_text)} characters\")\n",
    "    return markdown_text\n",
    "\n",
    "\n",
    "def chunk_markdown(\n",
    "    markdown_text: str,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    chunk_overlap: int = CHUNK_OVERLAP\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split Markdown into chunks with semantic boundaries.\n",
    "    \n",
    "    Args:\n",
    "        markdown_text: Markdown text to chunk\n",
    "        chunk_size: Maximum chunk size in characters\n",
    "        chunk_overlap: Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    # Define separators that respect Markdown structure\n",
    "    separators = [\n",
    "        \"\\n## \",      # H2 headers\n",
    "        \"\\n### \",     # H3 headers\n",
    "        \"\\n#### \",    # H4 headers\n",
    "        \"\\n\\n\",       # Paragraphs\n",
    "        \"\\n\",         # Lines\n",
    "        \". \",         # Sentences\n",
    "        \" \",          # Words\n",
    "        \"\"            # Characters\n",
    "    ]\n",
    "    \n",
    "    # Create text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=separators,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = text_splitter.create_documents([markdown_text])\n",
    "    print(f\"  Created {len(chunks)} chunks\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings using Granite embedding model.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array of embeddings\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def index_chunks_to_elasticsearch(\n",
    "    chunks: List[Document],\n",
    "    source_name: str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Index chunks with embeddings into Elasticsearch.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of LangChain Document objects\n",
    "        source_name: Name of source document\n",
    "    \n",
    "    Returns:\n",
    "        Number of documents indexed\n",
    "    \"\"\"\n",
    "    print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Extract text from chunks\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings(texts)\n",
    "    \n",
    "    print(f\"Indexing to Elasticsearch...\")\n",
    "    \n",
    "    # Prepare bulk indexing data\n",
    "    bulk_data = []\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        doc_id = f\"{source_name}_{i}\"\n",
    "        \n",
    "        bulk_data.append({\n",
    "            \"_index\": ES_INDEX_NAME,\n",
    "            \"_id\": doc_id,\n",
    "            \"_source\": {\n",
    "                \"text\": chunk.page_content,\n",
    "                \"embedding\": embedding.tolist(),\n",
    "                \"metadata\": chunk.metadata,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": i,\n",
    "                \"source\": source_name\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Bulk index\n",
    "    success, failed = helpers.bulk(es_client, bulk_data, refresh=True)\n",
    "    \n",
    "    print(f\"  Indexed: {success} documents\")\n",
    "    if failed:\n",
    "        print(f\"  Failed: {failed} documents\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Process PDFs - Complete Pipeline\n",
    "\n",
    "**What this does:** Runs the complete pipeline on your PDF documents.\n",
    "\n",
    "**Pipeline steps:**\n",
    "1. Convert PDF to Markdown (Docling using IBM Granite 4)\n",
    "2. Chunk Markdown into semantic segments\n",
    "3. Generate embeddings (Granite)\n",
    "4. Index to Elasticsearch\n",
    "\n",
    "**How to use:**\n",
    "- Add your PDF paths to the `pdf_documents` list\n",
    "- Supports both local files and URLs\n",
    "- Processes each document sequentially\n",
    "\n",
    "**Example PDF sources:**\n",
    "- Local file: `\"/path/to/your/document.pdf\"`\n",
    "- URL: `\"https://arxiv.org/pdf/2408.09869\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURE YOUR PDF DOCUMENTS HERE =====\n",
    "pdf_documents = [\n",
    "    # Example: Docling paper from arXiv\n",
    "     \"https://arxiv.org/pdf/2408.09869\",\n",
    "    # \"https://www.ibm.com/downloads/documents/us-en/1227c12d3a38b173\",\n",
    "    # Add your own PDFs here:\n",
    "     \"/Users/danielbenner/Documents/4 Archives/work/Seizing the AI and automation opportunity/SUMMARY_Seizing the AI and automation opportunity.pdf\",\n",
    "     \"/Users/danielbenner/Documents/4 Archives/work/INBOX/20240111_PresalesEngineering.pdf\",\n",
    "    # \"https://example.com/document.pdf\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(pdf_documents)} PDF documents...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_chunks_indexed = 0\n",
    "\n",
    "# Process each PDF\n",
    "for i, pdf_path in enumerate(pdf_documents, 1):\n",
    "    print(f\"\\n[{i}/{len(pdf_documents)}] Processing: {pdf_path}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Convert PDF to Markdown\n",
    "        markdown = convert_pdf_to_markdown(pdf_path)\n",
    "        \n",
    "        # Step 2: Chunk the Markdown\n",
    "        chunks = chunk_markdown(markdown)\n",
    "        \n",
    "        # Step 3 & 4: Generate embeddings and index to Elasticsearch\n",
    "        source_name = Path(pdf_path).stem if not pdf_path.startswith('http') else f\"doc_{i}\"\n",
    "        num_indexed = index_chunks_to_elasticsearch(chunks, source_name)\n",
    "        \n",
    "        total_chunks_indexed += num_indexed\n",
    "        print(f\"‚úì Completed processing {pdf_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error processing {pdf_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\n‚úì Pipeline Complete!\")\n",
    "print(f\"  Total documents processed: {len(pdf_documents)}\")\n",
    "print(f\"  Total chunks indexed: {total_chunks_indexed}\")\n",
    "\n",
    "# Verify index status\n",
    "count = es_client.count(index=ES_INDEX_NAME)['count']\n",
    "print(f\"  Documents in Elasticsearch: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Define Search Functions\n",
    "\n",
    "**What this does:** Creates functions for semantic and hybrid search.\n",
    "\n",
    "**Search capabilities:**\n",
    "\n",
    "### 1. `semantic_search(query, top_k, min_score)`\n",
    "- Pure vector similarity search\n",
    "- Uses cosine similarity\n",
    "- Best for finding conceptually similar content\n",
    "- Ignores exact keyword matches\n",
    "\n",
    "### 2. `hybrid_search(query, top_k, semantic_weight)`\n",
    "- Combines semantic + keyword search\n",
    "- `semantic_weight` controls the balance:\n",
    "  - 0.0 = Pure keyword search\n",
    "  - 0.5 = Equal weight\n",
    "  - 1.0 = Pure semantic search\n",
    "- Best for most production use cases\n",
    "\n",
    "**Parameters:**\n",
    "- `query`: Search query text\n",
    "- `top_k`: Number of results to return (default: 5)\n",
    "- `min_score`: Minimum similarity score (0.0-1.0)\n",
    "- `semantic_weight`: Weight for semantic search (0.0-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    min_score: float = 0.0\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using Granite embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        top_k: Number of results to return\n",
    "        min_score: Minimum similarity score threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of search results with scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [query],\n",
    "        normalize_embeddings=True\n",
    "    )[0]\n",
    "    \n",
    "    # Elasticsearch KNN search using script_score\n",
    "    search_query = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_embedding.tolist()}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"text\", \"source\", \"chunk_id\", \"metadata\"]\n",
    "    }\n",
    "    \n",
    "    response = es_client.search(index=ES_INDEX_NAME, body=search_query)\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        score = hit['_score'] - 1.0  # Adjust back to cosine similarity\n",
    "        if score >= min_score:\n",
    "            results.append({\n",
    "                'text': hit['_source']['text'],\n",
    "                'source': hit['_source']['source'],\n",
    "                'chunk_id': hit['_source']['chunk_id'],\n",
    "                'metadata': hit['_source'].get('metadata', {}),\n",
    "                'score': score\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    semantic_weight: float = 0.7\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform hybrid search combining semantic and keyword search.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        top_k: Number of results to return\n",
    "        semantic_weight: Weight for semantic search (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        List of search results with combined scores\n",
    "    \"\"\"\n",
    "    keyword_weight = 1.0 - semantic_weight\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [query],\n",
    "        normalize_embeddings=True\n",
    "    )[0]\n",
    "    \n",
    "    # Hybrid search query\n",
    "    search_query = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": [\n",
    "                            {\n",
    "                                \"match\": {\n",
    "                                    \"text\": {\n",
    "                                        \"query\": query,\n",
    "                                        \"boost\": keyword_weight\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": f\"({semantic_weight} * (cosineSimilarity(params.query_vector, 'embedding') + 1.0)) + _score\",\n",
    "                    \"params\": {\"query_vector\": query_embedding.tolist()}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"text\", \"source\", \"chunk_id\", \"metadata\"]\n",
    "    }\n",
    "    \n",
    "    response = es_client.search(index=ES_INDEX_NAME, body=search_query)\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        results.append({\n",
    "            'text': hit['_source']['text'],\n",
    "            'source': hit['_source']['source'],\n",
    "            'chunk_id': hit['_source']['chunk_id'],\n",
    "            'metadata': hit['_source'].get('metadata', {}),\n",
    "            'score': hit['_score']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úì Search functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Example - Semantic Search\n",
    "\n",
    "**What this does:** Demonstrates pure semantic search capabilities.\n",
    "\n",
    "**Use cases:**\n",
    "- Finding conceptually similar content\n",
    "- Question answering\n",
    "- Discovering related documents\n",
    "- Cross-lingual search (with multilingual models)\n",
    "\n",
    "**Try different queries:**\n",
    "- Conceptual: \"How does machine learning process documents?\"\n",
    "- Technical: \"What are the benefits of vector embeddings?\"\n",
    "- Exploratory: \"Methods for converting PDFs to structured data\"\n",
    "\n",
    "**Reading results:**\n",
    "- Score: 0.0-1.0 (higher = more similar)\n",
    "- Typical good matches: 0.6+\n",
    "- Excellent matches: 0.8+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURE YOUR SEARCH QUERY HERE =====\n",
    "search_query = \"How does Docling process PDF documents?\"\n",
    "num_results = 3\n",
    "\n",
    "print(f\"Semantic Search Query: '{search_query}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform search\n",
    "results = semantic_search(search_query, top_k=num_results)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    print(f\"\\nFound {len(results)} results:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  Score: {result['score']:.4f}\")\n",
    "        print(f\"  Source: {result['source']} (chunk {result['chunk_id']})\")\n",
    "        print(f\"  Text preview: {result['text'][:200]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Example - Hybrid Search\n",
    "\n",
    "**What this does:** Demonstrates hybrid search combining semantic + keyword matching.\n",
    "\n",
    "**When to use:**\n",
    "- Production search applications\n",
    "- When exact terms matter\n",
    "- Balancing precision and recall\n",
    "- User-facing search interfaces\n",
    "\n",
    "**Tuning semantic_weight:**\n",
    "- **0.9-1.0**: Mostly semantic (good for natural language queries)\n",
    "- **0.6-0.8**: Balanced (recommended default)\n",
    "- **0.3-0.5**: Keyword-heavy (good for technical term searches)\n",
    "- **0.0-0.2**: Mostly keyword (traditional search)\n",
    "\n",
    "**Pro tip:** Start with 0.7 and adjust based on your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURE YOUR SEARCH QUERY HERE =====\n",
    "search_query = \"table extraction from PDFs\"\n",
    "num_results = 3\n",
    "semantic_weight = 0.7  # Adjust between 0.0 (keyword) and 1.0 (semantic)\n",
    "\n",
    "print(f\"Hybrid Search Query: '{search_query}'\")\n",
    "print(f\"Semantic Weight: {semantic_weight} | Keyword Weight: {1-semantic_weight}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform search\n",
    "results = hybrid_search(search_query, top_k=num_results, semantic_weight=semantic_weight)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    print(f\"\\nFound {len(results)} results:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  Combined Score: {result['score']:.4f}\")\n",
    "        print(f\"  Source: {result['source']} (chunk {result['chunk_id']})\")\n",
    "        print(f\"  Text preview: {result['text'][:200]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Advanced - Batch Query Multiple Questions\n",
    "\n",
    "**What this does:** Demonstrates how to process multiple queries efficiently.\n",
    "\n",
    "**Use cases:**\n",
    "- Evaluating search quality\n",
    "- Testing different query formulations\n",
    "- Building Q&A systems\n",
    "- Automated content discovery\n",
    "\n",
    "**Add your queries:**\n",
    "- Modify the `queries` list\n",
    "- Each query is processed independently\n",
    "- Results are displayed separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURE YOUR QUERIES HERE =====\n",
    "queries = [\n",
    "    \"What is document conversion?\",\n",
    "    \"How to extract tables from documents?\",\n",
    "    \"Benefits of using embeddings for search\",\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(queries)} queries...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\nQuery {i}: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = semantic_search(query, top_k=2)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"Top result (score: {results[0]['score']:.4f}):\")\n",
    "        print(f\"  {results[0]['text'][:150]}...\")\n",
    "    else:\n",
    "        print(\"  No results found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì Batch query complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Statistics and Index Information\n",
    "\n",
    "**What this does:** Displays statistics about your indexed data.\n",
    "\n",
    "**Information shown:**\n",
    "- Total documents in index\n",
    "- Index size on disk\n",
    "- Number of unique sources\n",
    "- Sample document structure\n",
    "- Index health status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get document count\n",
    "doc_count = es_client.count(index=ES_INDEX_NAME)['count']\n",
    "print(f\"Total documents: {doc_count}\")\n",
    "\n",
    "# Get index stats\n",
    "stats = es_client.indices.stats(index=ES_INDEX_NAME)\n",
    "size_bytes = stats['indices'][ES_INDEX_NAME]['total']['store']['size_in_bytes']\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "print(f\"Index size: {size_mb:.2f} MB\")\n",
    "\n",
    "# Get unique sources\n",
    "agg_query = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"unique_sources\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"source\",\n",
    "                \"size\": 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "agg_results = es_client.search(index=ES_INDEX_NAME, body=agg_query)\n",
    "sources = agg_results['aggregations']['unique_sources']['buckets']\n",
    "\n",
    "print(f\"\\nUnique sources: {len(sources)}\")\n",
    "for source in sources:\n",
    "    print(f\"  - {source['key']}: {source['doc_count']} chunks\")\n",
    "\n",
    "# Sample document\n",
    "sample = es_client.search(index=ES_INDEX_NAME, size=1)['hits']['hits'][0]['_source']\n",
    "print(f\"\\nSample document structure:\")\n",
    "print(f\"  Text length: {len(sample['text'])} characters\")\n",
    "print(f\"  Embedding dimension: {len(sample['embedding'])}\")\n",
    "print(f\"  Source: {sample['source']}\")\n",
    "print(f\"  Chunk ID: {sample['chunk_id']}\")\n",
    "\n",
    "print(\"\\n‚úì Statistics retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Clean Up (Optional)\n",
    "\n",
    "**What this does:** Provides commands to clean up resources.\n",
    "\n",
    "**‚ö†Ô∏è WARNING:**\n",
    "- Uncommenting the delete command will **permanently delete** your index\n",
    "- All indexed data will be lost\n",
    "- Only run this if you want to start fresh\n",
    "\n",
    "**When to use:**\n",
    "- Finished with this demo\n",
    "- Want to reindex with different settings\n",
    "- Testing and experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clean Up Options\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ‚ö†Ô∏è UNCOMMENT THE FOLLOWING LINES TO DELETE THE INDEX ‚ö†Ô∏è\n",
    "# print(f\"Deleting index: {ES_INDEX_NAME}...\")\n",
    "# es_client.indices.delete(index=ES_INDEX_NAME)\n",
    "# print(\"‚úì Index deleted\")\n",
    "\n",
    "print(\"To delete the index, uncomment the lines in this cell.\")\n",
    "print(f\"Current index '{ES_INDEX_NAME}' is preserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What You've Built\n",
    "‚úì End-to-end RAG pipeline with:\n",
    "  - PDF preprocessing using Docling\n",
    "  - Semantic chunking with context preservation\n",
    "  - High-quality embeddings with Granite models\n",
    "  - Fast vector search with Elasticsearch\n",
    "\n",
    "### Key Features\n",
    "- ‚úì Preserves document structure (tables, images, layout)\n",
    "- ‚úì Intelligent chunking respects semantic boundaries\n",
    "- ‚úì Enterprise-ready embeddings with commercial licenses\n",
    "- ‚úì Hybrid search combining semantic + keyword matching\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Optimize for Your Use Case:**\n",
    "   - Adjust `CHUNK_SIZE` and `CHUNK_OVERLAP` for better results\n",
    "   - Experiment with different Granite models\n",
    "   - Tune `semantic_weight` in hybrid search\n",
    "\n",
    "2. **Scale Up:**\n",
    "   - Process larger document collections\n",
    "   - Implement batch processing for efficiency\n",
    "   - Add document deduplication\n",
    "\n",
    "3. **Enhance Search:**\n",
    "   - Add metadata filtering\n",
    "   - Implement re-ranking\n",
    "   - Add query expansion\n",
    "\n",
    "4. **Build Applications:**\n",
    "   - Connect to LLM for Q&A (Granite 4, GPT, etc.)\n",
    "   - Build web API with FastAPI\n",
    "   - Create interactive UI with Streamlit\n",
    "\n",
    "5. **Production Considerations:**\n",
    "   - Add error handling and logging\n",
    "   - Implement monitoring and metrics\n",
    "   - Set up index backups\n",
    "   - Configure Elasticsearch security\n",
    "\n",
    "### Resources\n",
    "- Docling: https://github.com/DS4SD/docling\n",
    "- Granite Models: https://www.ibm.com/granite\n",
    "- Elasticsearch: https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html\n",
    "\n",
    "### Questions?\n",
    "Review the detailed comments in each cell for guidance on configuration and usage.\n",
    "\n",
    "Happy building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-docling-granite-elastic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
